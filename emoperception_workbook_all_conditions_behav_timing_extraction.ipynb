{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib as plt \n",
        "# %matplotlib"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define paths\n",
        "\n",
        "behav_dir=\"/Volumes/wd_daelsaid/emo/CNI_EmoPerception/Emo_Data_090417_ALL 2\"\n",
        "# main_dir='/Volumes/daelsaid/data/emoperception_processing/17131'\n",
        "# behav_dir='1/data/emoperception_processing/happy_cleanbehav/temp/'\n",
        "#behav_dir='/Volumes/daelsaid/data/emoperception_processing/17131'\n",
        "#happy=glob(os.path.join(main_dir,'emo_2','*.log'))[0]\n",
        "#anger=glob(os.path.join(main_dir,'emo_1','*.log'))[0]\n",
        "\n",
        "files=(os.path.join(behav_dir,'17134_AmbiguousFaces_Happy_6-25-2017_2017_Aug_30_1328.log'))\n",
        "\n",
        "#create filename to be used for txt file of conditions\n",
        "\n\n",
        "filename=files.split('/')[-1].split('.')[0]\n",
        "subj=filename.split('_')[0]\n",
        "\n",
        "print filename,subj\n",
        "# for f in files:\n",
        "#     all_split=(re.split('; |_|/*|',f))\n",
        "#     filename.append([all_split[10]+'_'+all_split[12]+'_'])\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17134_AmbiguousFaces_Happy_6-25-2017_2017_Aug_30_1328 17134\n"
          ]
        }
      ],
      "execution_count": 20,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#read subject log lines into array\n",
        "\n",
        "# def emotion_lines(emotion_files):\n",
        "#     subj_log_lines=np.genfromtxt(emotion_files,delimiter=\"\\t\",dtype=str)\n",
        "#     return subj_log_lines\n",
        "\n\n\nsubj_log_lines=np.genfromtxt(files,delimiter=\"\\t\",dtype=str)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/Users/lillyel-said/anaconda2/lib/python2.7/site-packages/numpy/lib/npyio.py:1487: UserWarning: genfromtxt: Empty input file: \"/Volumes/wd_daelsaid/emo/CNI_EmoPerception/Emo_Data_090417_ALL 2/17134_AmbiguousFaces_Happy_6-25-2017_2017_Aug_30_1328.log\"\n",
            "  warnings.warn('genfromtxt: Empty input file: \"%s\"' % fname)\n"
          ]
        }
      ],
      "execution_count": 21,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get scan trigger and trial timings and different trials\n",
        "\n",
        "trials=[]\n",
        "trial_timing=[]\n",
        "scan_start=[]\n",
        "\n",
        "for idx,line in enumerate(subj_log_lines):\n",
        "    if line[2] == 'text_2: autoDraw = True':\n",
        "        scan_start=float(line[0])        \n",
        "    if line[2].endswith('autoDraw = True'):\n",
        "        trial_timing.append([line[0],line[1],line[2]])\n",
        "    if line[2].startswith('New trial'):\n",
        "        trials.append([line[0],line[2]])\n",
        "    "
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#extract duration of stimulus in seconds\n",
        "\n",
        "fix=np.array([x for x in trial_timing if x[2].startswith('FixCross')])\n",
        "image=np.array([x for x in trial_timing if x[2].startswith('Face')])\n",
        "mask=np.array([x for x in trial_timing if x[2].startswith('Mask')])\n",
        "\n\n",
        "f=fix[:,:][:,0].reshape(150,1).astype(float) \n",
        "i=image[:,:][:,0].reshape(150,1).astype(float)\n",
        "m=mask[:,:][:,0].reshape(150,1).astype(float) \n",
        "\n",
        "image_timing=np.subtract(m,i)\n",
        "iti=np.subtract(f[1:],f[:-1])\n"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#determine duration adn onsent time relative to scan start \n",
        "image_relative=np.subtract(i,scan_start)\n",
        "mask_relative=np.subtract(m,scan_start)"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#extract condition per trial, and percent emotion  of stimulus\n",
        "emotion_with_percent=[]\n",
        "index_with_cor=[]\n",
        "for_mat_file=[]\n",
        "for ix,trial in enumerate(trials):\n",
        "    index_with_cor.append([ix,trial[1].split(':')[3].split(\"['\")[1].split(\"']\")[0]])\n",
        "    percent_emotion=trial[1].split('{')[:][1].split(',')[4]\n",
        "    emotion=trial[1].split('{')[:][1].split(',')[0]\n",
        "    combined=(emotion.split(\"': u'\")[1]+'_'+percent_emotion.split(': ')[1]).replace(\"'\",'')\n",
        "    emotion_with_percent.append([ix,combined])\n",
        "    for_mat_file.append(combined)\n"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ix, image in enumerate(trials):\n",
        "    if 'bmp' in image[-1]:\n",
        "        print image"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#combine arrays to get 3 column array\n",
        "\n\n",
        "arr=np.append(image_timing,emotion_with_percent,axis=1)\n",
        "arr=np.append(image_relative,arr,axis=1)\n",
        "print np.delete(arr,2,axis=1)\n",
        "arr=np.delete(arr,2,axis=1)\n",
        "arr=np.delete(arr,0,axis=0)\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['4.2526' '0.0337' 'Anger_75']\n",
            " ['6.5533' '0.1009' 'Anger_75']\n",
            " ['8.7546' '0.1005' 'Anger_50']\n",
            " ['10.7556' '0.1003' 'Anger_100']\n",
            " ['13.1902' '0.0999' 'Anger_0']\n",
            " ['15.358' '0.0997' 'Anger_100']\n",
            " ['17.3589' '0.1005' 'Anger_25']\n",
            " ['19.4934' '0.0832' 'Anger_25']\n",
            " ['21.5278' '0.0832' 'Anger_25']\n",
            " ['23.9624' '0.0831' 'Anger_0']\n",
            " ['25.9634' '0.0832' 'Anger_75']\n",
            " ['27.9644' '0.0832' 'Anger_0']\n",
            " ['30.0822' '0.1001' 'Anger_50']\n",
            " ['32.4999' '0.1' 'Anger_0']\n",
            " ['34.8513' '0.0998' 'Anger_75']\n",
            " ['37.0524' '0.0998' 'Anger_75']\n",
            " ['39.0702' '0.0832' 'Anger_25']\n",
            " ['41.0879' '0.0998' 'Anger_50']\n",
            " ['43.3557' '0.0998' 'Anger_0']\n",
            " ['45.3567' '0.1003' 'Anger_50']\n",
            " ['47.5245' '0.0998' 'Anger_50']\n",
            " ['49.7589' '0.1' 'Anger_25']\n",
            " ['51.76' '0.0998' 'Anger_100']\n",
            " ['54.2279' '0.0831' 'Anger_25']\n",
            " ['56.229' '0.0831' 'Anger_100']\n",
            " ['58.5301' '0.0831' 'Anger_50']\n",
            " ['60.5311' '0.0832' 'Anger_25']\n",
            " ['62.5822' '0.0835' 'Anger_0']\n",
            " ['64.5833' '0.0831' 'Anger_50']\n",
            " ['66.5843' '0.0996' 'Anger_25']\n",
            " ['68.5853' '0.0998' 'Anger_100']\n",
            " ['70.5864' '0.0997' 'Anger_50']\n",
            " ['72.5873' '0.0998' 'Anger_75']\n",
            " ['74.5883' '0.0998' 'Anger_0']\n",
            " ['76.8895' '0.0999' 'Anger_100']\n",
            " ['78.8905' '0.0998' 'Anger_0']\n",
            " ['80.8916' '0.0997' 'Anger_50']\n",
            " ['83.126' '0.0997' 'Anger_25']\n",
            " ['85.1269' '0.1' 'Anger_0']\n",
            " ['87.3616' '0.0998' 'Anger_25']\n",
            " ['89.3459' '0.0831' 'Anger_100']\n",
            " ['91.347' '0.0831' 'Anger_50']\n",
            " ['93.3479' '0.0832' 'Anger_100']\n",
            " ['95.3489' '0.0832' 'Anger_75']\n",
            " ['97.45' '0.0832' 'Anger_100']\n",
            " ['99.4511' '0.0831' 'Anger_75']\n",
            " ['101.5688' '0.0998' 'Anger_75']\n",
            " ['103.5699' '0.0998' 'Anger_75']\n",
            " ['105.5709' '0.0998' 'Anger_0']\n",
            " ['107.8887' '0.0838' 'Anger_100']\n",
            " ['124.8976' '0.0834' 'Anger_25']\n",
            " ['126.8984' '0.0832' 'Anger_100']\n",
            " ['129.0662' '0.0832' 'Anger_25']\n",
            " ['131.1506' '0.083' 'Anger_100']\n",
            " ['133.285' '0.0833' 'Anger_50']\n",
            " ['135.5028' '0.0832' 'Anger_75']\n",
            " ['137.7374' '0.0998' 'Anger_0']\n",
            " ['139.8717' '0.0999' 'Anger_75']\n",
            " ['141.9393' '0.1' 'Anger_0']\n",
            " ['143.9405' '0.0999' 'Anger_50']\n",
            " ['145.9415' '0.0999' 'Anger_25']\n",
            " ['147.9592' '0.0998' 'Anger_0']\n",
            " ['150.3104' '0.0999' 'Anger_75']\n",
            " ['152.4615' '0.0999' 'Anger_25']\n",
            " ['154.9628' '0.0998' 'Anger_25']\n",
            " ['156.9639' '0.0997' 'Anger_0']\n",
            " ['159.1149' '0.0833' 'Anger_75']\n",
            " ['161.1159' '0.0832' 'Anger_25']\n",
            " ['163.4504' '0.0833' 'Anger_100']\n",
            " ['165.5849' '0.0832' 'Anger_100']\n",
            " ['167.5859' '0.0832' 'Anger_100']\n",
            " ['170.0372' '0.0832' 'Anger_100']\n",
            " ['172.4051' '0.0998' 'Anger_75']\n",
            " ['174.823' '0.0999' 'Anger_0']\n",
            " ['176.8241' '0.0998' 'Anger_0']\n",
            " ['178.8251' '0.0998' 'Anger_75']\n",
            " ['181.0428' '0.0999' 'Anger_25']\n",
            " ['183.0605' '0.0999' 'Anger_0']\n",
            " ['185.0782' '0.0999' 'Anger_50']\n",
            " ['187.0959' '0.0999' 'Anger_100']\n",
            " ['189.1136' '0.0999' 'Anger_75']\n",
            " ['191.1479' '0.0999' 'Anger_75']\n",
            " ['193.5159' '0.0998' 'Anger_0']\n",
            " ['195.8337' '0.0832' 'Anger_50']\n",
            " ['198.3183' '0.0831' 'Anger_100']\n",
            " ['200.3194' '0.083' 'Anger_100']\n",
            " ['202.3203' '0.0832' 'Anger_75']\n",
            " ['204.3214' '0.0833' 'Anger_0']\n",
            " ['206.3224' '0.0832' 'Anger_50']\n",
            " ['208.3235' '0.0998' 'Anger_50']\n",
            " ['210.3912' '0.0999' 'Anger_50']\n",
            " ['212.3922' '0.0998' 'Anger_100']\n",
            " ['214.3932' '0.0999' 'Anger_50']\n",
            " ['216.3943' '0.0998' 'Anger_50']\n",
            " ['218.412' '0.0999' 'Anger_75']\n",
            " ['220.4297' '0.0998' 'Anger_25']\n",
            " ['222.4474' '0.0998' 'Anger_50']\n",
            " ['224.4651' '0.0999' 'Anger_25']\n",
            " ['226.4993' '0.0999' 'Anger_0']\n",
            " ['228.9673' '0.0832' 'Anger_25']\n",
            " ['246.1097' '0.1007' 'Anger_25']\n",
            " ['248.1105' '0.1' 'Anger_0']\n",
            " ['250.1116' '0.0999' 'Anger_75']\n",
            " ['252.396' '0.0999' 'Anger_75']\n",
            " ['254.7141' '0.0997' 'Anger_0']\n",
            " ['257.0318' '0.0999' 'Anger_75']\n",
            " ['259.0493' '0.1' 'Anger_0']\n",
            " ['261.4173' '0.0999' 'Anger_100']\n",
            " ['263.4184' '0.0999' 'Anger_100']\n",
            " ['265.4027' '0.0832' 'Anger_100']\n",
            " ['267.4037' '0.0832' 'Anger_0']\n",
            " ['269.4048' '0.0831' 'Anger_50']\n",
            " ['271.4391' '0.0833' 'Anger_25']\n",
            " ['273.4402' '0.0832' 'Anger_0']\n",
            " ['275.5078' '0.0833' 'Anger_25']\n",
            " ['277.9258' '0.0999' 'Anger_75']\n",
            " ['279.9268' '0.0999' 'Anger_25']\n",
            " ['282.2781' '0.0999' 'Anger_25']\n",
            " ['284.4791' '0.0999' 'Anger_100']\n",
            " ['286.4801' '0.0999' 'Anger_75']\n",
            " ['288.4979' '0.0999' 'Anger_75']\n",
            " ['290.749' '0.0999' 'Anger_50']\n",
            " ['293.0669' '0.1' 'Anger_100']\n",
            " ['295.0846' '0.0998' 'Anger_50']\n",
            " ['297.5525' '0.0999' 'Anger_75']\n",
            " ['299.5369' '0.0831' 'Anger_50']\n",
            " ['301.5545' '0.0833' 'Anger_75']\n",
            " ['303.8558' '0.0831' 'Anger_25']\n",
            " ['305.8568' '0.0832' 'Anger_100']\n",
            " ['307.8578' '0.0832' 'Anger_0']\n",
            " ['309.8588' '0.0832' 'Anger_75']\n",
            " ['311.8598' '0.0832' 'Anger_25']\n",
            " ['313.8609' '0.0999' 'Anger_100']\n",
            " ['316.1453' '0.0999' 'Anger_50']\n",
            " ['318.1464' '0.0999' 'Anger_50']\n",
            " ['320.6143' '0.0999' 'Anger_50']\n",
            " ['323.0322' '0.0999' 'Anger_100']\n",
            " ['325.3167' '0.0999' 'Anger_100']\n",
            " ['327.3345' '0.0999' 'Anger_50']\n",
            " ['329.5524' '0.0997' 'Anger_50']\n",
            " ['331.5865' '0.0999' 'Anger_25']\n",
            " ['333.5877' '0.0998' 'Anger_0']\n",
            " ['335.7554' '0.0832' 'Anger_0']\n",
            " ['338.0066' '0.0832' 'Anger_25']\n",
            " ['340.0076' '0.0832' 'Anger_0']\n",
            " ['342.3588' '0.0832' 'Anger_25']\n",
            " ['344.3598' '0.0832' 'Anger_0']\n",
            " ['346.3609' '0.0832' 'Anger_100']\n",
            " ['348.3618' '0.0999' 'Anger_75']\n",
            " ['350.3629' '0.0999' 'Anger_50']]\n"
          ]
        }
      ],
      "execution_count": 17,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#break apart array by condition/percent emotion\n",
        "\n",
        "happy_0=[]\n",
        "happy_25=[]\n",
        "happy_50=[]\n",
        "happy_75=[]\n",
        "happy_100=[]\n",
        "\n",
        "for index,emo in enumerate(arr):\n",
        "    if '_0' in emo[:][2:][0]:\n",
        "        happy_0.append(emo)\n",
        "    if '_25' in emo[:][2:][0]:\n",
        "        happy_25.append(emo)\n",
        "    if '_50' in emo[:][2:][0]:\n",
        "        happy_50.append(emo)\n",
        "    if '_75' in emo[:][2:][0]:\n",
        "        happy_75.append(emo)\n",
        "        print happy_75\n",
        "    if '_100' in emo[:][2:][0]:\n",
        "        happy_100.append(emo)        "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#generate mat file of each condition adn regressors\n",
        "\n",
        "# import scipy.io as scp\n",
        "scp.savemat('test.mat',dict({'duration':image_timing,'onsets':image_relative,'names':['happy','anger']}))\n",
        "# arr.getfield()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write array to text file\n",
        "\n",
        "def write_arr_tofile(array, file_with_ext,savemat):\n",
        "    with open(file_with_ext,\"w+\") as myfile:\n",
        "        np.savetxt(file_with_ext,array,delimiter=' ',fmt='%s')\n",
        "    "
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "write_arr_tofile(arr,'17134_a.txt',0)\n",
        "# write_arr_tofile(happy_0,os.path.join(behav_dir,filename+'_happy0.txt'),1)\n",
        "# write_arr_tofile(happy_25,os.path.join(behav_dir,filename+'_happy25.txt'),1)\n",
        "# write_arr_tofile(happy_50,os.path.join(behav_dir,filename+'_happy50.txt'),1)\n",
        "# write_arr_tofile(happy_75,os.path.join(behav_dir,filename+'_happy75.txt'),1)\n",
        "# write_arr_tofile(happy_100,os.path.join(behav_dir,filename+'_happy100.txt'),1)"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print os.getcwd()\n",
        "import scipy.io as scp\n",
        "\n",
        "print arr[0:][0:]\n",
        "            \n",
        "# scp.savemat(os.path.join(behav_dir,'output',subj+'_happy0.txt'),happy_0,appendmat=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print subj_log_lines\n",
        "actual=[]\n",
        "for idx,line in enumerate(subj_log_lines):\n",
        "    if 'theseKeys' in line:\n",
        "        print line\n",
        "    if line[1].startswith('DATA'):\n",
        "        actual.append(line[2].split(': ')[1])\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_corr=[]\n",
        "\n",
        "for y in range(len(index_with_cor)):\n",
        "    print y\n",
        "#     print [index_with_cor[x][0],index_with_cor[x][1]==actual[x]]\n",
        "#     response_corr.append([index_with_cor[x][0],index_with_cor[x][1]==actual[x]])\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print 'face --> mask:',(np.subtract(float(fix[0][0]),float(mask[0][0])))\n",
        "# print 'mask --> fixation:',(np.subtract(float(fix[0][0]),float(image[0][0])))\n",
        "# print 'ISI, mask --> image:',(np.subtract(float(mask[0][0]),float(image[0][0])))\n",
        "# print 'number of trials:', len(trials)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "inputHidden": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_emoperception_conditions(behav_data_path,file,output_txt_path):\n",
        "    behav_dir=behav_data_path\n",
        "    output_dir=output_txt_path\n",
        "    filename=file.split('/')[-1]\n",
        "\n",
        "    #define paths\n",
        "    # file=glob(os.path.join(behav_dir,'*.log'))[2]\n",
        "\n",
        "    #read subject log lines\n",
        "\n",
        "    subj_log_lines=np.genfromtxt(file,delimiter=\"\\t\",dtype=str)\n",
        "\n",
        "    #get scan trigger and trial timings and different trials\n",
        "\n",
        "    trials=[]\n",
        "    trial_timing=[]\n",
        "    scan_start=[]\n",
        "\n",
        "    for idx,line in enumerate(subj_log_lines):\n",
        "        if line[2] == 'text_2: autoDraw = True':\n",
        "            scan_start=float(line[0])\n",
        "        if line[2].endswith('autoDraw = True'):\n",
        "            trial_timing.append([line[0],line[1],line[2]])\n",
        "        if line[2].startswith('New trial'):\n",
        "            trials.append([line[0],line[2]])\n",
        "\n",
        "    #extract duration of stimulus in seconds\n",
        "\n",
        "    fix=np.array([x for x in trial_timing if x[2].startswith('FixCross')])\n",
        "    image=np.array([x for x in trial_timing if x[2].startswith('Face')])\n",
        "    mask=np.array([x for x in trial_timing if x[2].startswith('Mask')])\n",
        "\n\n",
        "    f=fix[:,:][:,0].reshape(150,1).astype(float)\n",
        "    i=image[:,:][:,0].reshape(150,1).astype(float)\n",
        "    m=mask[:,:][:,0].reshape(150,1).astype(float)\n",
        "\n",
        "    image_timing=np.subtract(m,i)\n",
        "    iti=np.subtract(f[1:],f[:-1])\n",
        "\n",
        "    #determine duration adn onsent time relative to scan start\n",
        "    image_relative=np.subtract(i,scan_start)\n",
        "    mask_relative=np.subtract(m,scan_start)\n",
        "\n",
        "    #extract condition per trial, and percent emotion  of stimulus\n",
        "    emotion_with_percent=[]\n",
        "    index_with_cor=[]\n",
        "\n",
        "    for ix,trial in enumerate(trials):\n",
        "        index_with_cor.append([ix,trial[1].split(':')[3].split(\"['\")[1].split(\"']\")[0]])\n",
        "        percent_emotion=trial[1].split('{')[:][1].split(',')[4]\n",
        "        emotion=trial[1].split('{')[:][1].split(',')[0]\n",
        "        combined=(emotion.split(\"': u'\")[1]+'_'+percent_emotion.split(': ')[1]).replace(\"'\",'')\n",
        "        emotion_with_percent.append([ix,combined])\n",
        "        #combine arrays to get 3 column array\n",
        "\n\n",
        "    #create new array with 3 columns\n",
        "    arr=np.append(image_timing,emotion_with_percent,axis=1)\n",
        "    arr=np.append(image_relative,arr,axis=1)\n",
        "    # print np.delete(arr,2,axis=1)\n",
        "    arr=np.delete(arr,2,axis=1)\n",
        "    arr=np.delete(arr,0,axis=0)\n",
        "\n\n",
        "    #break apart array by condition/percent emotion\n",
        "\n",
        "    happy_0=[]\n",
        "    happy_25=[]\n",
        "    happy_50=[]\n",
        "    happy_75=[]\n",
        "    happy_100=[]\n",
        "\n",
        "    for index,emo in enumerate(arr):\n",
        "        if '_0' in emo[:][2:][0]:\n",
        "            happy_0.append(emo)\n",
        "        if '_25' in emo[:][2:][0]:\n",
        "            happy_25.append(emo)\n",
        "        if '_50' in emo[:][2:][0]:\n",
        "            happy_50.append(emo)\n",
        "        if '_75' in emo[:][2:][0]:\n",
        "            happy_75.append(emo)\n",
        "        if '_100' in emo[:][2:][0]:\n",
        "            happy_100.append(emo)\n",
        "\n",
        "    #write array to text file\n",
        "\n",
        "    def write_arr_tofile(array, file_with_ext):\n",
        "        with open(file_with_ext,\"w+\") as myfile:\n",
        "            np.savetxt(file_with_ext,array,delimiter=' ',fmt='%s')\n",
        "\n\n\n",
        "    write_arr_tofile(happy_0,os.path.join(behav_dir,output_dir,filename+'_happy0.txt'))\n",
        "    write_arr_tofile(happy_25,os.path.join(behav_dir,output_dir,filename+'_happy25.txt'))\n",
        "    write_arr_tofile(happy_50,os.path.join(behav_dir,output_dir,filename+'_happy50.txt'))\n",
        "    write_arr_tofile(happy_75,os.path.join(behav_dir,output_dir,filename+'_happy75.txt'))\n",
        "    write_arr_tofile(happy_100,os.path.join(behav_dir,output_dir,filename+'_happy100.txt'))\n",
        "\n",
        "    actual=[]\n",
        "    for idx,line in enumerate(subj_log_lines):\n",
        "        if 'theseKeys' in line[2]:\n",
        "            print line[2]\n",
        "        if line[1].startswith('DATA'):\n",
        "            actual.append(line[2].split(': ')[1])\n",
        "\n",
        "    response_corr=[]\n",
        "\n",
        "    for y in range(len(index_with_cor)):\n",
        "        print y\n",
        "    #     print [index_with_cor[x][0],index_with_cor[x][1]==actual[x]]\n",
        "    #     response_corr.append([index_with_cor[x][0],index_with_cor[x][1]==actual[x]])\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "inputHidden": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# behav_path=('/Users/lillyel-said/Desktop/stanford/scripts/jupyter_notebooks/anger_good')\n",
        "# curr_dir = os.getcwd()\n",
        "# outputpath=os.path.join(behav_path,'output')\n",
        "\n",
        "# current_dir=[f for f in os.listdir('.') if os.path.isfile(f)]\n",
        "\n",
        "# for f in current_dir:\n",
        "#     filename, ext = os.path.splitext(f)\n",
        "#     if ext=='.log':\n",
        "#         logfile = os.path.join(curr_dir,f)\n",
        "#         try:\n",
        "#             extract_emoperception_conditions(behav_path,logfile,outputpath)\n",
        "#         except:\n",
        "#             pass\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "inputHidden": true
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python2",
      "language": "python",
      "display_name": "Python 2"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 2,
        "name": "ipython"
      }
    },
    "kernel_info": {
      "name": "python2"
    },
    "nteract": {
      "version": "0.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}